{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40714873",
   "metadata": {},
   "source": [
    "ChromaDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3df1a382",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFDirectoryLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3bb4348f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cv.pdf', 'model.pdf', 'vector.ipynb']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.listdir(\"/krish Naik/Code files/Github_GenAi/GenAi_Code/vectorDB/\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "24a31649",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_loader=PyPDFDirectoryLoader(\"\\\\krish Naik\\\\Code files\\\\Github_GenAi\\\\GenAi_Code\\\\vectorDB\\\\\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dba46a3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(directory_loader.load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ea285e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = directory_loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "85467283",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2025-04-12T15:15:52+05:00', 'author': 'Uzair Khan', 'moddate': '2025-04-12T15:15:52+05:00', 'source': '\\\\krish Naik\\\\Code files\\\\Github_GenAi\\\\GenAi_Code\\\\vectorDB\\\\cv.pdf', 'total_pages': 2, 'page': 0, 'page_label': '1'}, page_content='Muhammad Uzair \\nData Scientist & GenAI Engineer \\n uzairu471@gmail.com  +92 3248562515  Islamabad, Pakistan  15 Jun 2003 \\n LinkedIn  Github \\nPROFILE \\nWith 1 years of experience in Machine learning , Deep Lea rning and Generative AI, I have \\nsuccessfully implemented NLP and Generative AI projects using the MLOps Pipeline to make \\ndata-driven decisions. I excel in Machine learning, Neural Networks, Transformers, Python, \\nLang Chain, Llama Index, RAG, Vector Databases, Hugging  Face, and building reliable LLM \\napplications. Seeking to leverage my expertise to contribute to a forward -thinking team and \\ndrive impactful solutions in a dynamic, tech -driven environment. \\nSKILLS \\nPython: (DSA), Statistics, Deep Learning: (ANN | Pytorch)                                                             \\nMachine learning: Stacking Models, Boosting Models, Classification, Regression, Data Preprocessing, Feature \\nengineering, Pandas, Numpy, Scikit-learn.                                                                                                           \\nNatural Language Processing: (RNN | LSTM | Encoder Decoder | Attention |Transformer | BERT | GPT)  \\nGenerative AI: (Open AI, Mistral , Llama, Gemini, Embeddings, HuggingFace, RAG, Finetuning, AI Agents)       \\nLLM Framework: (Langchain | LlamaIndex)                                                                                                        \\nVector Databases: (FAISS | ChromaDB | Pinecone)                                                                                        \\nRestAPIs: (FastAPI)                                                                                                                                                  \\nFrontEnd:  (ReactJs)                                                                                                                                                       \\nDatabases: (MySQL | SQL |  MongoDB)                                                                                                                                   \\nCloud: (AWS | Azure)                                                                                                                                                  \\nVersion Control: (Git, Github) \\nPROFESSIONAL  EXPERIENCE \\n  Research Assistant, SmartLab-ICP                                                               Nov 2024 – Present | Peshawar, PK \\n• Published research on Urdu Fake News Detection using advanced machine learning and deep \\nlearning techniques. \\n• Fine-tuned LLMs for code generation using Parameter-Efficient Fine-Tuning methods like LoRA \\nand QLoRA. \\n• Built and fine-tuned transformer models (BERT, GPT, T5) and RAG-based systems for QA and \\nretrieval tasks. \\n• Applied deep learning with RNN, LSTM, and GRU models using TensorFlow and scikit-learn for \\nNLP solutions. \\n• Designed and deployed scalable AI/ML solutions for real-world NLP and multimodal applications \\nusing Python. \\n• Implemented GenAI workflows using LangChain, FAISS, Weaviate, and OpenAI APIs for RAG \\nand autonomous agents. \\n• Performed robust model training, tuning, and evaluation with tools like TensorBoard, MLflow, and \\nW&B. \\n• Integrated AI pipelines into backend systems with FastAPI, enabling cloud-ready deployment \\n(AWS). \\nMachine Learning Intern, CodeVenator         Jun 2023 – Oct 2024 | Peshawar, PK  \\n• Developed interactive Streamlit dashboards for classification and regression model deployment. \\n• Built robust web scraping pipelines to extract and structure data from diverse online sources. \\n• Executed comprehensive data preprocessing, cleaning, and augmentation to ensure consistency \\nacross formats. \\n• Worked on Market Sales prediction and data analysis projects.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2025-04-12T15:15:52+05:00', 'author': 'Uzair Khan', 'moddate': '2025-04-12T15:15:52+05:00', 'source': '\\\\krish Naik\\\\Code files\\\\Github_GenAi\\\\GenAi_Code\\\\vectorDB\\\\cv.pdf', 'total_pages': 2, 'page': 1, 'page_label': '2'}, page_content='PROJECTS \\nDemand Letter Automation for Sales Force \\n  Tech: Python, Pandas, FastAPI, ReactJs, FPDF, CSV Integration. \\nSolution Build: \\n• Developed an intelligent system to generate demand letters dynamically based on sales data, quotas, \\nand territory performance. \\n• Designed user-friendly interfaces through ReactJs for data input and letter customization, \\nstreamlining communication between management and the sales force. \\nFake News Detection Application \\n Tech: Python, scikit-learn, PyTorch, Streamlit, NLTK, Hugging Face Transformers. \\nSolution Build: \\n• Developed an end-to-end application to detect fake news using machine learning and deep learning \\ntechniques. \\n• Fine-tuned transformer models (BERT) on multilingual datasets including Urdu and English for high \\naccuracy. \\n• Designed a user interface to input news content and receive real-time predictions on its authenticity. \\nHONORS AND AWARDS \\nGenerative AI Course – Krish Naik \\nAdvanced Python Programming Course and Certificate – NAVTTC \\nMachine Learning Specialization – Coursera (by Andrew Ng) \\nDeep Learning Specialization – Coursera (by Andrew Ng) \\nNatural Language Processing (NLP) –Youtube (CampusX) \\nPython Programming Certification – Pearson VUE \\nEDUCATION \\n \\nBS Computer Science \\nIslamia College and University Peshawar \\n CGPA: 3.51 \\nJul 2021 – Jul 2025 | Peshawar, KPK \\nRelevant Coursework: DSA, DBMS, OS, Artifical Intelligence, Internet Programming, Netw ork Security, \\nJava, C++'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 9.26', 'creator': '', 'creationdate': '2022-04-25T20:08:24-07:00', 'moddate': '2022-04-25T20:08:24-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'source': '\\\\krish Naik\\\\Code files\\\\Github_GenAi\\\\GenAi_Code\\\\vectorDB\\\\model.pdf', 'total_pages': 6, 'page': 0, 'page_label': '1'}, page_content='Learning Models for Urdu Fake News Detection\\nFazlourrahman Balouchzahi, H L Shashirekha\\nDepartment of Computer Science, Mangalore University, Mangalore - 574199, India\\nAbstract\\nDetecting fake news from the real news can be modeled as a typical binary text classification problem.\\nMost of the models proposed for fake news detection address the resource rich languages such as English\\nand Spanish but, languages such as Urdu, Persian, Balouchi and many Indian native languages have\\nreceived very less attention due to unavailability of bench marked corpus. To promote text processing\\nactivities on Urdu, which happens to be a resource poor language FIRE 2020 (Forum for Information\\nRetrieval Evaluation) has called for UrduFake, a shared task to detect fake news in Urdu language.\\nHigh speed of news broadcast and the importance of detecting fake news from the real news made us\\n(team MUCS) to propose three different learning models namely, an ensemble of Machine Learning (ML)\\nmodels, Transfer Learning (TL) model based on ULMFiT and a hybrid model made up of an ensemble\\nof ML approaches, TL approach and Deep Learning (DL). The proposed methodology utilizes word and\\ncharacter n-grams to train ML model and word embedding vectors to train BiLSTM networks of DL\\nmodel and for TL model, a pre-trained general domain Urdu Language Model is fine-tuned with the\\nUrdu fake news dataset. Our ML model obtained 5th place among 9 teams that participated in this task.\\nKeywords\\nFake news Detection, Learning Models, BiLSTM, ULMFiT\\n1. Introduction\\nToday the speed of broadcasting news is increasing rapidly due to the availability of various\\nonline platforms and social media such as Facebook, Twitter, WhatsApp etc. Online platforms\\nserve as a great opportunity for fake news spreaders to manipulate communities’ minds and\\nalso social trust [\\n1] due to anonymity of users. Fake news can target unity of people in the\\nsociety and also can impact the society in a negative way. Detecting the ever increasing fake\\nnews manually is laborious, time consuming and error prone. Further, as news articles are\\nunstructured text and usually noisy, efficient approaches are required to detect fake news au-\\ntomatically [\\n2]. Most of the proposed fake news detection tasks have addressed resource rich\\nlanguages such as English and Spanish [ 3].But, resource poor languages such as Urdu, Persian,\\nBalouchi and many Indian native languages have received less attention due to unavailability\\nor less availability of labeled data. To promote text processing activities on Urdu, FIRE 2020\\nhas called for UrduFake, a shared task to detect fake news in Urdu language [\\n4][5]. Fake news\\ndetection can be modeled as a typical binary text classification problem where each news arti-\\ncle is classified as either fake or real [\\n6]. In this paper, we, team MUCS, propose three different\\nFIRE 2020: Forum for Information Retrieval Evaluation, December 16-20, 2020, Hyderabad, India\\n/envelope-open\\nfrs_b@yahoo.com (F. Balouchzahi); hlsrekha@gmail.com (H.L. Shashirekha)\\n/globehttps://mangaloreuniversity.ac.in/dr-h-l-shashirekha (H.L. Shashirekha)\\n/orcid0000-0003-1937-3475 (F. Balouchzahi)\\n© 2020 Copyright for this paper by its authors.\\nUse permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0).\\nCEUR\\nWor k s hop\\nPr oc eedi ngs \\nht t p: / / c eur - ws . or g\\nI SSN 1613- 0073\\nCEUR Workshop Proceedings ( CEUR-WS.org)'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 9.26', 'creator': '', 'creationdate': '2022-04-25T20:08:24-07:00', 'moddate': '2022-04-25T20:08:24-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'source': '\\\\krish Naik\\\\Code files\\\\Github_GenAi\\\\GenAi_Code\\\\vectorDB\\\\model.pdf', 'total_pages': 6, 'page': 1, 'page_label': '2'}, page_content='models namely, an ensemble of Machine Learning (ML) models, Transfer Learning (TL) based\\non ULMFiT and a hybrid model made up of an ensemble of ML models, TL learning model and\\nDeep Learning (DL) model for Urdu fake news detection.\\n2. Literature Review\\nFake news detection is a challenging task particularly for resource poor languages. Due to\\nunavailability or less availability of bench marked corpus, several researchers have created\\ntheir own datasets and have developed various models to detect fake news. Some of the relevant\\nworks are mentioned below: A DL model based on LSTM networks to detect false news from\\nTwitter and news article proposed by Bilal et. al. [\\n1] use emotions to illustrate that false\\ninformation can be detected based on the combination of different emotional patterns. They\\nhave reported an f1 score of 96% on a dataset including trusted news created from English Gig\\nword corpus as real news and collection of news from seven different unreliable news sites\\nas false news. Urdu fake news detection proposed by Ajmad et. al. [\\n3] have used Machine\\nTranslation (MT) to translate English fake news dataset consisting of 200 legitimate and 200\\nfake news [\\n7] to Urdu and combined it with an original Urdu dataset that contains 500 real\\nand 400 fake news [ 8]. Using character and word n-grams to train Support Vector Machine the\\nauthors have reported that the results on original Urdu dataset with f1 score ranging from 0.83\\nto 0.89 are higher than that of the f1 score obtained for the dataset through MT. Two models\\nbased on different learning approaches for English and Spanish languages have been submitted\\nto fake news spreader detection at PAN 2020\\n1 shared task by Shashirekha et. al. [ 9] [ 10] an, i)\\nan ensemble of ML models using majority voting of the three (two Linear SVC classifiers and a\\nLogistic Regression classifier) classifiers built using Unigram TF/IDF, N_gram TF and Doc2Vec\\nfeature sets and ii) a TL model based on Universal Language Model Fine-Tuning (ULMFiT)\\ninitially trained on a general domain English/Spanish data collected from Wikipedia which is\\nthen fine-tuned using target task dataset and used for the fake news spreader detection task as\\nthe target model. Trained on the dataset provided by PAN 2020 [\\n11], the ML model obtained\\n73.50% and 67.50% accuracies and TL model 62% and 64% accuracies on English and Spanish\\nlanguages respectively.\\n3. Methodology\\nWe propose three different learning models for Urdu fake news detection, namely, i) an en-\\nsemble of ML models trained with word and character n-grams, ii) TL model based on ULMFiT\\nusing a pre-trained Urdu Language Model (LM) fine-tuned with Urdu fake news dataset and\\niii) HTC - a hybrid model made up of models used in i), ii) and a DL model trained with word\\nembedding vectors. The framework of HTC model is shown in Figure 1.\\nThe base models used for the proposed approaches are described below:\\n(i) Ensemble of ML models:Three ML models, namely, Multinomial Naïve Bayes (MNB),\\nMultilayer Perceptron (MLP), and Logistic Regression (LR) are ensembled using ‘hard’\\n1https://pan.webis.de/'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 9.26', 'creator': '', 'creationdate': '2022-04-25T20:08:24-07:00', 'moddate': '2022-04-25T20:08:24-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'source': '\\\\krish Naik\\\\Code files\\\\Github_GenAi\\\\GenAi_Code\\\\vectorDB\\\\model.pdf', 'total_pages': 6, 'page': 2, 'page_label': '3'}, page_content='Figure 1:Framework of HTC model.\\nFigure 2:Architecture of ensemble of ML models.\\nvoting. All the three models are trained on vectors obtained using CountVectorizer mod-\\nule from word n-grams (n= 1, 2) and char n-grams (n=1, 2, 3, 4, 5). For MLP, hidden layer\\nsizes are set to (150, 100, 50) and maximum iteration, activation, solver, and random state\\nhave been set to 300, Relu, Adam and 1 respectively and for MNB and LR classifiers default\\nparameters are used.Figure 2 gives the architecture of ensemble of ML models.\\n(ii) DL model: It has been implemented using a pre-trained Skipgram word embedding\\nmodel trained on Wikipedia texts and the parameters used for training are: \"alpha: 0.05,\\n\"hs\": 0, \"iter\": 15, \"max_n\": 5, \"min_count\": 50, \"min_n\": 2, \"negative\": 20, \"sample\": 0.0001,\\n\"sg\": 1, \"size\": 300, \"window\": 10, \"word_ngrams\": 1\\n2. Word embeddings are used to build\\nembedding matrix for the given dataset which is used to train a multi-channel BiLSTM\\nnetwork of three channels with similar configuration as Conv1D (200, 3, activation=’relu’,\\npadding=’same’)\\n3. The model has been trained in 20 epochs each with a batch of size 256,\\n128, 64, and 32. Figure 3 shows the architecture of DL model.\\n(iii) TL model: It consists of three stages namely, Language Model (LM) training, LM fine-\\ntuning, and target task classifier. LM is a probability distribution over word sequences\\nin a language. In TL model, the knowledge obtained in solving one task called source\\ntask is used to develop another task, called the target task [\\n12] [ 13].In the proposed TL\\n2https://github.com/urduhack/urdu-word-vectors\\n3A 1D Convolutional Neural Networks CNN is very effective for deriving features from a fixed-length segment\\nof the overall dataset, where it is not so important where the feature is located in the segment.'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 9.26', 'creator': '', 'creationdate': '2022-04-25T20:08:24-07:00', 'moddate': '2022-04-25T20:08:24-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'source': '\\\\krish Naik\\\\Code files\\\\Github_GenAi\\\\GenAi_Code\\\\vectorDB\\\\model.pdf', 'total_pages': 6, 'page': 3, 'page_label': '4'}, page_content='Figure 3:Architecture of DL model.\\nFigure 4:Transfer Learning model frame work.\\nmodel based on ULMFiT, source model is a pre-trained general domain Urdu LM 4 that\\nrepresents the general features of Urdu language and target model is a fake news detection\\nmodel. The pre-trained LM is fine-tuned with the target task dataset for Urdu fake news\\ndetection. TL model is implemented based on ULMFiT architecture introduced by Howard\\net. al. [\\n14] and target classifier using text.models module from fastai library. Inspired by\\nStephen et. al. [ 15], an encoder for an ASGD Weight-Dropped LSTM (AWD-LSTM) is\\nimplemented which can be plugged in with a decoder and classifying layers to create a\\ntext classifier. AWD-LSTM has shown noticeable results on word-level models consisting\\nof a word embedding of size 400, 3 hidden layers and 1150 hidden activations per layer\\n[\\n14]. A framework of TL model is shown in Figure 4.\\n4. Experimental Results\\nTrain and test data are pre-processed by removing punctuation, stopwords, numbers and un-\\nnecessary characters such as @, , $, %. Classifier models are constructed using the respective\\nfeatures extracted by the feature engineering module. Test data is classified based on the ma-\\njority voting of the predicted labels in case of ensemble of ML models and HTC model.\\n4.1. Dataset\\nThe training and development corpus called Bend-The-Truth data consisting of Fake and Real\\nnews provided by UrduFake\\n5 task organizers are shown in Table 1. Dataset consists of Urdu\\nnews articles collected from various channels such as BBC Urdu News, CNN Urdu, Express-\\nNews, Jung News, Naway Waqat, and some other news websites [\\n3]. Further, 400 news articles\\nare provided by the organizers as private test set for evaluating the learning models.\\n4.2. Results\\nThe labels for the test data predicted by the three proposed models are submitted to UrduFake\\nshared task organizers and the results reported by organizers are shown in Table 2. Among\\n4https://github.com/anuragshas/nlp-for-urdu\\n5https://www.urdufake2020.cicling.org/home'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 9.26', 'creator': '', 'creationdate': '2022-04-25T20:08:24-07:00', 'moddate': '2022-04-25T20:08:24-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'source': '\\\\krish Naik\\\\Code files\\\\Github_GenAi\\\\GenAi_Code\\\\vectorDB\\\\model.pdf', 'total_pages': 6, 'page': 4, 'page_label': '5'}, page_content='Table 1\\nStatistics of the corpus used for training and development set\\nCategory Business Health Showbiz Sports Technology Total\\nReal 100 100 100 100 100 500\\nFake 50 10 100 50 100 400\\nTable 2\\nResults of our proposed models\\nModels Fake Real F1 avg. AccuracyP R F1 mac. P R F1 mac.\\nEnsemble of ML 0.7833 0.6266 0.7707 0.8000 0.8960 0.7707 0.7894 0.7950\\nTL 0.5918 0.3866 0.6143 0.6953 0.8400 0.6143 0.6509 0.6700\\nHTC 0.7956 0.4933 0.7192 0.7524 0.9240 0.7192 0.7467 0.7625\\nFigure 5:Comparison of accuracy and F1 average of the models submitted by 9 teams\\nthe three proposed models, ensemble of ML models obtained higher results compared to other\\ntwo models with an average f1 score of 0.7894. Also, our team, MUCS, obtained 5th rank in\\nUrduFake challenge among the 9 participating teams. The higher performance for ensemble\\nof ML models is due to n-grams features that have already proved their effectiveness in many\\nworks in NLP. TL model has obtained less performance from what was expected because of a\\ngeneral domain LM used as pre-trained LM instead of domain specific pre-trained LM. Further,\\nthe lower performance of DL model is may be because only word embeddings are used as\\nfeatures. The lower performances of DL and TL models have resulted in lower performance of\\nHTC model. A comparison of accuracy and F1 average of the models submitted by the 9 teams\\nis shown in Figure 5.\\n5. Conclusion and Future work\\nWe, team MUCS, proposed three different learning models namely, an ensemble of ML models,\\nTL model based on ULMFiT and HTC - a hybrid model made up of an ensemble of ML models,\\nTL model based on ULMFiT and DL model for the detection of UrduFake news task in FIRE 2020.'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 9.26', 'creator': '', 'creationdate': '2022-04-25T20:08:24-07:00', 'moddate': '2022-04-25T20:08:24-07:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'source': '\\\\krish Naik\\\\Code files\\\\Github_GenAi\\\\GenAi_Code\\\\vectorDB\\\\model.pdf', 'total_pages': 6, 'page': 5, 'page_label': '6'}, page_content='Our team, obtained 5 th rank for ensemble of ML models among the 9 participating teams. We\\nwould like to explore different features and improve learning models and perform experiments\\non native and low resource languages such as Urdu, Persian and other Indian languages.\\nReferences\\n[1] B. Ghanem, P. Rosso, F. Rangel, An emotional analysis of false information in social media\\nand news articles, ACM Transactions on Internet Technology (TOIT) 20 (2020) 1ś18.\\n[2] J. Tang, Y. Chang, H. Liu, Mining social media with social theories: a survey, ACM Sigkdd\\nExplorations Newsletter 15 (2014) 20ś29.\\n[3] M. Amjad, G. Sidorov, A. Zhila, Data augmentation using machine translation for fake\\nnews detection in the urdu language, in: Proceedings of The 12th Language Resources\\nand Evaluation Conference, 2020, pp. 2537ś2542.\\n[4] M. Amjad, G. Sidorov, A. Zhila, P. Rosso, A. Gelbukh, Urdufake@fire2020: Overview of the\\ntrack on fake news detection in urdu, In Proceedings of the 12th Forum for Information\\nRetrieval Evaluation. (2020).\\n[5] M. Amjad, G. Sidorov, A. Zhila, A. Gelbukh, P. Rosso, Overview of the shared task on fake\\nnews detection in urdu at fire 2020, CEUR Workshop Proceedings (2020). Working Notes\\nof the Forum for Information Retrieval Evaluation (FIRE 2020), Hyderabad, India.\\n[6] C. Aggarwal, C. Zhai, A survey of text classification algorithms in mining text data (2012)\\n163ś222.\\n[7] V. Pérez-Rosas, B. Kleinberg, A. Lefevre, R. Mihalcea, Automatic detection of fake news,\\narXiv preprint arXiv:1708.07104 (2017).\\n[8] M. Amjad, G. Sidorov, A. Zhila, H. Gómez-Adorno, I. Voronkov, A. Gelbukh, łbend the\\ntruthž: Benchmark dataset for fake news detection in urdu language and its evaluation,\\nJournal of Intelligent & Fuzzy Systems (2020) 1ś13.\\n[9] M. D. Anusha, H. L. Shashirekha, N. S. Prakash, Ensemble model for profiling fake news\\nspreaders on twitter - notebook for pan at clef 2020, In Linda Cappellato, CarstenEick-\\nhoff, Nicola Ferro, and AurélieNévéol, editors, CLEF 2020 Labs and Workshops, Notebook\\nPapers, CEUR-WS.org (2020).\\n[10] F. Balouchzahi, H. L. Shashirekha, Ulmfit for twitter fake news spreader profiling - note-\\nbook for pan at clef 2020, In Linda Cappellato, CarstenEickhoff, Nicola Ferro, and Au-\\nrélieNévéol, editors, CLEF 2020 Labs and Workshops, Notebook Papers, CEUR-WS.org\\n(2020).\\n[11] F. Rangel, A. Giachanou, B. Ghanem, P. Rosso, Overview of the 8th author profiling task\\nat pan 2020: profiling fake news spreaders on twitter, in: CLEF, 2020.\\n[12] F. Balouchzahi, H. L. Shashirekha, PUNER-Parsi ULMFiT for Named-Entity Recognition\\nin Persian Texts, Technical Report, EasyChair, 2020.\\n[13] S. Faltl, M. Schimpke, C. Hackober, Ulmfit: State-of-the-art in text analysis (2019).\\n[14] J. Howard, S. Ruder, Universal language model fine-tuning for text classification, arXiv\\npreprint arXiv:1801.06146 (2018).\\n[15] S. Merity, N. S. Keskar, R. Socher, Regularizing and optimizing lstm language models,\\narXiv preprint arXiv:1708.02182 (2017).')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b7486fd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Muhammad Uzair \\nData Scientist & GenAI Engineer \\n uzairu471@gmail.com  +92 3248562515  Islamabad, Pakistan  15 Jun 2003 \\n LinkedIn  Github \\nPROFILE \\nWith 1 years of experience in Machine learning , Deep Lea rning and Generative AI, I have \\nsuccessfully implemented NLP and Generative AI projects using the MLOps Pipeline to make \\ndata-driven decisions. I excel in Machine learning, Neural Networks, Transformers, Python, \\nLang Chain, Llama Index, RAG, Vector Databases, Hugging  Face, and building reliable LLM \\napplications. Seeking to leverage my expertise to contribute to a forward -thinking team and \\ndrive impactful solutions in a dynamic, tech -driven environment. \\nSKILLS \\nPython: (DSA), Statistics, Deep Learning: (ANN | Pytorch)                                                             \\nMachine learning: Stacking Models, Boosting Models, Classification, Regression, Data Preprocessing, Feature \\nengineering, Pandas, Numpy, Scikit-learn.                                                                                                           \\nNatural Language Processing: (RNN | LSTM | Encoder Decoder | Attention |Transformer | BERT | GPT)  \\nGenerative AI: (Open AI, Mistral , Llama, Gemini, Embeddings, HuggingFace, RAG, Finetuning, AI Agents)       \\nLLM Framework: (Langchain | LlamaIndex)                                                                                                        \\nVector Databases: (FAISS | ChromaDB | Pinecone)                                                                                        \\nRestAPIs: (FastAPI)                                                                                                                                                  \\nFrontEnd:  (ReactJs)                                                                                                                                                       \\nDatabases: (MySQL | SQL |  MongoDB)                                                                                                                                   \\nCloud: (AWS | Azure)                                                                                                                                                  \\nVersion Control: (Git, Github) \\nPROFESSIONAL  EXPERIENCE \\n  Research Assistant, SmartLab-ICP                                                               Nov 2024 – Present | Peshawar, PK \\n• Published research on Urdu Fake News Detection using advanced machine learning and deep \\nlearning techniques. \\n• Fine-tuned LLMs for code generation using Parameter-Efficient Fine-Tuning methods like LoRA \\nand QLoRA. \\n• Built and fine-tuned transformer models (BERT, GPT, T5) and RAG-based systems for QA and \\nretrieval tasks. \\n• Applied deep learning with RNN, LSTM, and GRU models using TensorFlow and scikit-learn for \\nNLP solutions. \\n• Designed and deployed scalable AI/ML solutions for real-world NLP and multimodal applications \\nusing Python. \\n• Implemented GenAI workflows using LangChain, FAISS, Weaviate, and OpenAI APIs for RAG \\nand autonomous agents. \\n• Performed robust model training, tuning, and evaluation with tools like TensorBoard, MLflow, and \\nW&B. \\n• Integrated AI pipelines into backend systems with FastAPI, enabling cloud-ready deployment \\n(AWS). \\nMachine Learning Intern, CodeVenator         Jun 2023 – Oct 2024 | Peshawar, PK  \\n• Developed interactive Streamlit dashboards for classification and regression model deployment. \\n• Built robust web scraping pipelines to extract and structure data from diverse online sources. \\n• Executed comprehensive data preprocessing, cleaning, and augmentation to ensure consistency \\nacross formats. \\n• Worked on Market Sales prediction and data analysis projects.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fd443214",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=2000,\n",
    "    chunk_overlap=300,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "77e8366a",
   "metadata": {},
   "outputs": [],
   "source": [
    "further_split_doc=text_splitter.split_documents(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6122cf00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(further_split_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5fd70ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4bb4726b",
   "metadata": {},
   "outputs": [],
   "source": [
    "persist_directory=\"vdb_latest\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "83fb0ade",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e0033ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94f64e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d968e152",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = OpenAIEmbeddings(model=\"text-embedding-3-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f684953e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OpenAIEmbeddings(client=<openai.resources.embeddings.Embeddings object at 0x000001B36E1F7BF0>, async_client=<openai.resources.embeddings.AsyncEmbeddings object at 0x000001B36E245850>, model='text-embedding-3-large', dimensions=None, deployment='text-embedding-ada-002', openai_api_version=None, openai_api_base=None, openai_api_type=None, openai_proxy=None, embedding_ctx_length=8191, openai_api_key=SecretStr('**********'), openai_organization=None, allowed_special=None, disallowed_special=None, chunk_size=1000, max_retries=2, request_timeout=None, headers=None, tiktoken_enabled=True, tiktoken_model_name=None, show_progress_bar=False, model_kwargs={}, skip_empty=False, default_headers=None, default_query=None, retry_min_seconds=4, retry_max_seconds=20, http_client=None, http_async_client=None, check_embedding_ctx_length=True)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8e395de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "255572e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cab89204",
   "metadata": {},
   "outputs": [],
   "source": [
    "chroma_vdb=Chroma.from_documents(documents=further_split_doc,\n",
    "                      embedding=embedding,\n",
    "                      persist_directory=persist_directory,\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2c4b34f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rahim\\AppData\\Local\\Temp\\ipykernel_5192\\3808622788.py:1: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  chroma_vdb.persist()\n"
     ]
    }
   ],
   "source": [
    "chroma_vdb.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36256d68",
   "metadata": {},
   "source": [
    "LOADING THE DATABASE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a79e566f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rahim\\AppData\\Local\\Temp\\ipykernel_5192\\1531868625.py:1: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  vdb=Chroma(persist_directory=persist_directory,\n"
     ]
    }
   ],
   "source": [
    "vdb=Chroma(persist_directory=persist_directory,\n",
    "        embedding_function=embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bbd3af06",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever=vdb.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ef450e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = retriever.get_relevant_documents(\"what is transformer and how it is working for llama2 model?\",k=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "efd5254b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'GPL Ghostscript 9.26', 'keywords': '', 'page': 2, 'page_label': '3', 'creator': '', 'author': '', 'creationdate': '2022-04-25T20:08:24-07:00', 'moddate': '2022-04-25T20:08:24-07:00', 'title': '', 'subject': '', 'source': '\\\\krish Naik\\\\Code files\\\\Github_GenAi\\\\GenAi_Code\\\\vectorDB\\\\model.pdf', 'total_pages': 6}, page_content='Figure 1:Framework of HTC model.\\nFigure 2:Architecture of ensemble of ML models.\\nvoting. All the three models are trained on vectors obtained using CountVectorizer mod-\\nule from word n-grams (n= 1, 2) and char n-grams (n=1, 2, 3, 4, 5). For MLP, hidden layer\\nsizes are set to (150, 100, 50) and maximum iteration, activation, solver, and random state\\nhave been set to 300, Relu, Adam and 1 respectively and for MNB and LR classifiers default\\nparameters are used.Figure 2 gives the architecture of ensemble of ML models.\\n(ii) DL model: It has been implemented using a pre-trained Skipgram word embedding\\nmodel trained on Wikipedia texts and the parameters used for training are: \"alpha: 0.05,\\n\"hs\": 0, \"iter\": 15, \"max_n\": 5, \"min_count\": 50, \"min_n\": 2, \"negative\": 20, \"sample\": 0.0001,\\n\"sg\": 1, \"size\": 300, \"window\": 10, \"word_ngrams\": 1\\n2. Word embeddings are used to build\\nembedding matrix for the given dataset which is used to train a multi-channel BiLSTM\\nnetwork of three channels with similar configuration as Conv1D (200, 3, activation=’relu’,\\npadding=’same’)\\n3. The model has been trained in 20 epochs each with a batch of size 256,\\n128, 64, and 32. Figure 3 shows the architecture of DL model.\\n(iii) TL model: It consists of three stages namely, Language Model (LM) training, LM fine-\\ntuning, and target task classifier. LM is a probability distribution over word sequences\\nin a language. In TL model, the knowledge obtained in solving one task called source\\ntask is used to develop another task, called the target task [\\n12] [ 13].In the proposed TL\\n2https://github.com/urduhack/urdu-word-vectors\\n3A 1D Convolutional Neural Networks CNN is very effective for deriving features from a fixed-length segment\\nof the overall dataset, where it is not so important where the feature is located in the segment.')]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3ff69838",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'similarity'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.search_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4d0961d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.search_kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b18dc00c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "240b6139",
   "metadata": {},
   "outputs": [
    {
     "ename": "DefaultCredentialsError",
     "evalue": "Your default credentials were not found. To set up Application Default Credentials, see https://cloud.google.com/docs/authentication/external/set-up-adc for more information.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mDefaultCredentialsError\u001b[0m                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m embedding \u001b[38;5;241m=\u001b[39m \u001b[43mGoogleGenerativeAIEmbeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodels/embedding-001\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "    \u001b[1;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\Rahim\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_google_genai\\embeddings.py:95\u001b[0m, in \u001b[0;36mGoogleGenerativeAIEmbeddings.validate_environment\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     92\u001b[0m     google_api_key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgoogle_api_key\n\u001b[0;32m     93\u001b[0m client_info \u001b[38;5;241m=\u001b[39m get_client_info(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGoogleGenerativeAIEmbeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 95\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_generative_service\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     96\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcredentials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcredentials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     97\u001b[0m \u001b[43m    \u001b[49m\u001b[43mapi_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgoogle_api_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     98\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclient_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclient_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     99\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclient_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    100\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    101\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Rahim\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_google_genai\\_genai_extension.py:276\u001b[0m, in \u001b[0;36mbuild_generative_service\u001b[1;34m(credentials, api_key, client_options, client_info, transport)\u001b[0m\n\u001b[0;32m    262\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbuild_generative_service\u001b[39m(\n\u001b[0;32m    263\u001b[0m     credentials: Optional[credentials\u001b[38;5;241m.\u001b[39mCredentials] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    264\u001b[0m     api_key: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    267\u001b[0m     transport: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    268\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m v1betaGenerativeServiceClient:\n\u001b[0;32m    269\u001b[0m     config \u001b[38;5;241m=\u001b[39m _prepare_config(\n\u001b[0;32m    270\u001b[0m         credentials\u001b[38;5;241m=\u001b[39mcredentials,\n\u001b[0;32m    271\u001b[0m         api_key\u001b[38;5;241m=\u001b[39mapi_key,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    274\u001b[0m         client_info\u001b[38;5;241m=\u001b[39mclient_info,\n\u001b[0;32m    275\u001b[0m     )\n\u001b[1;32m--> 276\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mv1betaGenerativeServiceClient\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Rahim\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\google\\ai\\generativelanguage_v1beta\\services\\generative_service\\client.py:696\u001b[0m, in \u001b[0;36mGenerativeServiceClient.__init__\u001b[1;34m(self, credentials, transport, client_options, client_info)\u001b[0m\n\u001b[0;32m    687\u001b[0m     transport_init: Union[\n\u001b[0;32m    688\u001b[0m         Type[GenerativeServiceTransport],\n\u001b[0;32m    689\u001b[0m         Callable[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, GenerativeServiceTransport],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    693\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m cast(Callable[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, GenerativeServiceTransport], transport)\n\u001b[0;32m    694\u001b[0m     )\n\u001b[0;32m    695\u001b[0m     \u001b[38;5;66;03m# initialize with the provided callable or the passed in class\u001b[39;00m\n\u001b[1;32m--> 696\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transport \u001b[38;5;241m=\u001b[39m \u001b[43mtransport_init\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    697\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcredentials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcredentials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    698\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcredentials_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client_options\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcredentials_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    699\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhost\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_api_endpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    700\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscopes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client_options\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscopes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    701\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclient_cert_source_for_mtls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client_cert_source\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    702\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquota_project_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client_options\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquota_project_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    703\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclient_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclient_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    704\u001b[0m \u001b[43m        \u001b[49m\u001b[43malways_use_jwt_access\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    705\u001b[0m \u001b[43m        \u001b[49m\u001b[43mapi_audience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client_options\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapi_audience\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    706\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    708\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124masync\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transport):\n\u001b[0;32m    709\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m CLIENT_LOGGING_SUPPORTED \u001b[38;5;129;01mand\u001b[39;00m _LOGGER\u001b[38;5;241m.\u001b[39misEnabledFor(\n\u001b[0;32m    710\u001b[0m         std_logging\u001b[38;5;241m.\u001b[39mDEBUG\n\u001b[0;32m    711\u001b[0m     ):  \u001b[38;5;66;03m# pragma: NO COVER\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Rahim\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\google\\ai\\generativelanguage_v1beta\\services\\generative_service\\transports\\grpc.py:235\u001b[0m, in \u001b[0;36mGenerativeServiceGrpcTransport.__init__\u001b[1;34m(self, host, credentials, credentials_file, scopes, channel, api_mtls_endpoint, client_cert_source, ssl_channel_credentials, client_cert_source_for_mtls, quota_project_id, client_info, always_use_jwt_access, api_audience)\u001b[0m\n\u001b[0;32m    230\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ssl_channel_credentials \u001b[38;5;241m=\u001b[39m grpc\u001b[38;5;241m.\u001b[39mssl_channel_credentials(\n\u001b[0;32m    231\u001b[0m                 certificate_chain\u001b[38;5;241m=\u001b[39mcert, private_key\u001b[38;5;241m=\u001b[39mkey\n\u001b[0;32m    232\u001b[0m             )\n\u001b[0;32m    234\u001b[0m \u001b[38;5;66;03m# The base transport sets the host, credentials and scopes\u001b[39;00m\n\u001b[1;32m--> 235\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    236\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhost\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhost\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    237\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcredentials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcredentials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    238\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcredentials_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcredentials_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    239\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscopes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscopes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    240\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquota_project_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquota_project_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    241\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclient_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclient_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    242\u001b[0m \u001b[43m    \u001b[49m\u001b[43malways_use_jwt_access\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malways_use_jwt_access\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    243\u001b[0m \u001b[43m    \u001b[49m\u001b[43mapi_audience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mapi_audience\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    244\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_grpc_channel:\n\u001b[0;32m    247\u001b[0m     \u001b[38;5;66;03m# initialize with the provided callable or the default channel\u001b[39;00m\n\u001b[0;32m    248\u001b[0m     channel_init \u001b[38;5;241m=\u001b[39m channel \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mcreate_channel\n",
      "File \u001b[1;32mc:\\Users\\Rahim\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\google\\ai\\generativelanguage_v1beta\\services\\generative_service\\transports\\base.py:100\u001b[0m, in \u001b[0;36mGenerativeServiceTransport.__init__\u001b[1;34m(self, host, credentials, credentials_file, scopes, quota_project_id, client_info, always_use_jwt_access, api_audience, **kwargs)\u001b[0m\n\u001b[0;32m     96\u001b[0m     credentials, _ \u001b[38;5;241m=\u001b[39m google\u001b[38;5;241m.\u001b[39mauth\u001b[38;5;241m.\u001b[39mload_credentials_from_file(\n\u001b[0;32m     97\u001b[0m         credentials_file, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mscopes_kwargs, quota_project_id\u001b[38;5;241m=\u001b[39mquota_project_id\n\u001b[0;32m     98\u001b[0m     )\n\u001b[0;32m     99\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m credentials \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ignore_credentials:\n\u001b[1;32m--> 100\u001b[0m     credentials, _ \u001b[38;5;241m=\u001b[39m \u001b[43mgoogle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mauth\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdefault\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    101\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mscopes_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquota_project_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquota_project_id\u001b[49m\n\u001b[0;32m    102\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    103\u001b[0m     \u001b[38;5;66;03m# Don't apply audience if the credentials file passed from user.\u001b[39;00m\n\u001b[0;32m    104\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(credentials, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwith_gdch_audience\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\Rahim\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\google\\auth\\_default.py:719\u001b[0m, in \u001b[0;36mdefault\u001b[1;34m(scopes, request, quota_project_id, default_scopes)\u001b[0m\n\u001b[0;32m    711\u001b[0m             _LOGGER\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[0;32m    712\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo project ID could be determined. Consider running \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    713\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`gcloud config set project` or setting the \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    714\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menvironment variable\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    715\u001b[0m                 environment_vars\u001b[38;5;241m.\u001b[39mPROJECT,\n\u001b[0;32m    716\u001b[0m             )\n\u001b[0;32m    717\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m credentials, effective_project_id\n\u001b[1;32m--> 719\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mDefaultCredentialsError(_CLOUD_SDK_MISSING_CREDENTIALS)\n",
      "\u001b[1;31mDefaultCredentialsError\u001b[0m: Your default credentials were not found. To set up Application Default Credentials, see https://cloud.google.com/docs/authentication/external/set-up-adc for more information."
     ]
    }
   ],
   "source": [
    "embedding = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eebab5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pinecone(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
